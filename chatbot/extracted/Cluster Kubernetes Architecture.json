{
  "filename": "Cluster Kubernetes Architecture.pdf",
  "content": "Highly Available Kubernetes \nCluster\n💡 C’est quoi Kubernetes ?\nHighly Available Kubernetes Cluster\n1\n\nKubernetes (ou K8s pour les intimes), c’est comme un chef d’orchestre qui gère \ndes applications sur plein de machines.\nTu veux lancer une appli ? Il la déploie, la surveille, redémarre si elle tombe, et \ns’occupe de la faire tourner là où il y a de la place.\n🧱 C’est quoi un cluster Kubernetes ?\nUn cluster, c’est un groupe de machines (physiques ou virtuelles) qui bossent \nensemble pour héberger des applications.\nY'a des chefs (masters) qui pilotent\nEt des ouvriers (workers) qui font tourner les applications\n🌐 Pourquoi utiliser un cluster multi-nœuds hautement \ndisponible ?\nPour éviter que tout plante si une machine tombe en panne.\nAvec plusieurs nœuds, Kubernetes peut continuer à fonctionner même si une \npartie du cluster a un souci.\n➡️ Disponibilité continue, résilience, et scalabilité.\n🧠 Les composants du master node (plan de contrôle)\nCe sont les cerveaux du cluster :\nAPI Server : La porte d’entrée, c’est lui qui reçoit toutes les commandes.\nScheduler : Il décide où lancer les applications.\nController Manager : Il surveille que tout fonctionne bien.\netcd : C’est la base de données du cluster (état global du système).\n💪 Les composants des worker nodes (nœuds de travail)\nCe sont les muscles :\nKubelet : Il suit les ordres du master.\nKube-proxy : Il gère le réseau.\nHighly Available Kubernetes Cluster\n2\n\nRuntime de conteneur (comme Docker) : Il fait tourner les applis.\n🧩 HAProxy et Keepalived, c’est quoi ?\nHAProxy : répartit les requêtes entre les serveurs de contrôle ( kube-apiserver ).\nKeepalived : fournit une IP virtuelle → si une machine tombe, une autre prend \nle relais automatiquement.\n❓ Pourquoi 3 nœuds master ? Pas 1, ni 2 ?\nC’est quoi un quorum ?\nUn quorum, c’est le nombre minimum de nœuds (dans un groupe) qui doivent \nêtre en ligne et d’accord pour que le système puisse prendre des décisions \n(écrire, élire un leader, etc.)\n🎯 En Kubernetes, le quorum est utilisé par etcd, la base de \ndonnées des masters.\nFormule du quorum :\nquorum = (nombre total de nœuds / 2) + 1\nLe quorum est toujours une majorité.\nSans quorum, etcd est bloqué ➜ donc le cluster Kubernetes est figé !\nDifférence entre 2 masters et 3 masters\n🧠 Cas\nNombre total de masters\nQuorum requis\nTolérance aux pannes\n❌ 2 masters\n2\n2\n0 (aucune panne possible)\n✅ 3 masters\n3\n2\n1 panne possible\nExemple avec 2 nœuds masters\nmaster1  et master2\nHighly Available Kubernetes Cluster\n3\n\nQuorum = 2\nSi 1 tombe, il ne reste qu’un seul nœud → quorum non atteint ❌\n➡️ Cluster bloqué même s’il te reste un master\nExemple avec 3 nœuds masters\nmaster1 , master2 , master3\nQuorum = 2\nSi 1 tombe (peu importe lequel), il reste 2 nœuds → quorum ok ✅\n➡️ Cluster continue à fonctionner normalement\n🌍 C’est quoi l’Ingress ?\nL’Ingress est comme un portier d’hôtel 🛎️. Il gère les routes HTTP (ex: /admin , \n/app ) et les redirige vers les bonnes applis.\n🧠 Exemple :\nwww.monsite.com/admin   → Pod A\nwww.monsite.com/shop    → Pod B\n👨‍💻 Comment un utilisateur accède à l’application ?\n1. Il tape une URL (ex : myapp.com )\n2. Le load balancer externe redirige vers l’Ingress\n3. L’Ingress envoie la requête vers le bon pod (ex: application Joget)\n4. Le pod répond → l'utilisateur voit son interface\n⚙️ Aider un DevOps à configurer les VM (pour Joget + DB)\nVoici une proposition simple et réaliste :\nRôle VM\nCPU\nRAM\nStockage\nDétails\nMaster Node\n(x3)\n2\n2 Go\n40 Go\netcd + control\nplane\nHighly Available Kubernetes Cluster\n4\n\nWorker Node\n(x2+)\n4\n8 Go\n60 Go\nJoget + apps\nBase de\ndonnées (DB)\n4\n8 Go\n60 Go\nMySQL/Postgres\nLoad Balancer\n(x2)\n1\n1 Go\n20 Go\nHAProxy +\nKeepalived\nNFS Server\n(option)\n1\n2 Go\n100 Go\nStockage partagé\n🔢 Les 3 Load Balancers dans ton cluster Kubernetes \nHA :\n1. 🌐 Load Balancer Externe\nRôle : Point d'entrée pour les utilisateurs finaux (navigateur, mobile, API, etc.)\nIP publique/virtuelle : ex. 192.168.1.100  ou myapp.company.com\nRedirige vers : les 2 load balancers internes (HAProxy)\nSouvent géré par : Keepalived (pour failover) + floating IP\n2. 🔁 Load Balancer Interne 1 (HAProxy 1) et Interne 2 (HAProxy \n2)\nRôle : répartissent le trafic vers les kube-apiserver  des 3 nœuds de contrôle\nIls sont en redondance grâce à Keepalived\nIls assurent que le plan de contrôle reste disponible même si un LB interne \ntombe\n📌 Comment fonctionne un cluster Kubernetes avec plusieurs \nmasters ?\nDans une architecture multi-masters, plusieurs serveurs (nœuds de contrôle) \nhébergent chacun une copie des composants critiques de Kubernetes ( kube-\napiserver , etcd , scheduler , etc.).\nHighly Available Kubernetes Cluster\n5\n\n👉 Cela permet au cluster de continuer à fonctionner même si un ou deux masters \ntombent en panne. C’est le principe de la haute disponibilité (HA).\n⚖️ Pourquoi faut-il un Load Balancer interne ?\nLe Load Balancer interne agit comme un point d’entrée unique vers les kube-\napiserver  répartis sur les masters.\nTous les composants (kubelet, kubectl, etc.) communiquent avec cette seule \nadresse (ex: 192.168.193.135:6443 ), sans se soucier de quel master est actif.\n✅ Ses avantages :\n🔁 Répartit le trafic entre les masters\n💥 Si un master tombe, les autres prennent le relais automatiquement\n🧩 Simplifie la configuration : une seule IP pour accéder au cluster\n❌ Et si on n’en met pas ?\nSans Load Balancer :\nTu dois taper manuellement sur les IPs des masters\nPas de bascule automatique en cas de panne\nLe cluster devient fragile et perd le bénéfice de la haute dispo\nHighly Available Kubernetes Cluster\n6\n\n"
}