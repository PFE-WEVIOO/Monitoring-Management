{
  "filename": "Cluster Kubernetes Architecture.pdf",
  "content": "Highly Available Kubernetes \nCluster\nğŸ’¡ Câ€™est quoi Kubernetes ?\nHighly Available Kubernetes Cluster\n1\n\nKubernetes (ou K8s pour les intimes), câ€™est comme un chef dâ€™orchestre qui gÃ¨re \ndes applications sur plein de machines.\nTu veux lancer une appli ? Il la dÃ©ploie, la surveille, redÃ©marre si elle tombe, et \nsâ€™occupe de la faire tourner lÃ  oÃ¹ il y a de la place.\nğŸ§± Câ€™est quoi un cluster Kubernetes ?\nUn cluster, câ€™est un groupe de machines (physiques ou virtuelles) qui bossent \nensemble pour hÃ©berger des applications.\nY'a des chefs (masters) qui pilotent\nEt des ouvriers (workers) qui font tourner les applications\nğŸŒ Pourquoi utiliser un cluster multi-nÅ“uds hautement \ndisponible ?\nPour Ã©viter que tout plante si une machine tombe en panne.\nAvec plusieurs nÅ“uds, Kubernetes peut continuer Ã  fonctionner mÃªme si une \npartie du cluster a un souci.\nâ¡ï¸ DisponibilitÃ© continue, rÃ©silience, et scalabilitÃ©.\nğŸ§  Les composants du master node (plan de contrÃ´le)\nCe sont les cerveaux du cluster :\nAPI Server : La porte dâ€™entrÃ©e, câ€™est lui qui reÃ§oit toutes les commandes.\nScheduler : Il dÃ©cide oÃ¹ lancer les applications.\nController Manager : Il surveille que tout fonctionne bien.\netcd : Câ€™est la base de donnÃ©es du cluster (Ã©tat global du systÃ¨me).\nğŸ’ª Les composants des worker nodes (nÅ“uds de travail)\nCe sont les muscles :\nKubelet : Il suit les ordres du master.\nKube-proxy : Il gÃ¨re le rÃ©seau.\nHighly Available Kubernetes Cluster\n2\n\nRuntime de conteneur (comme Docker) : Il fait tourner les applis.\nğŸ§© HAProxy et Keepalived, câ€™est quoi ?\nHAProxy : rÃ©partit les requÃªtes entre les serveurs de contrÃ´le ( kube-apiserver ).\nKeepalived : fournit une IP virtuelle â†’ si une machine tombe, une autre prend \nle relais automatiquement.\nâ“ Pourquoi 3 nÅ“uds master ? Pas 1, ni 2 ?\nCâ€™est quoi un quorum ?\nUn quorum, câ€™est le nombre minimum de nÅ“uds (dans un groupe) qui doivent \nÃªtre en ligne et dâ€™accord pour que le systÃ¨me puisse prendre des dÃ©cisions \n(Ã©crire, Ã©lire un leader, etc.)\nğŸ¯ En Kubernetes, le quorum est utilisÃ© par etcd, la base de \ndonnÃ©es des masters.\nFormule du quorum :\nquorum = (nombre total de nÅ“uds / 2) + 1\nLe quorum est toujours une majoritÃ©.\nSans quorum, etcd est bloquÃ© âœ donc le cluster Kubernetes est figÃ© !\nDiffÃ©rence entre 2 masters et 3 masters\nğŸ§  Cas\nNombre total de masters\nQuorum requis\nTolÃ©rance aux pannes\nâŒ 2 masters\n2\n2\n0 (aucune panne possible)\nâœ… 3 masters\n3\n2\n1 panne possible\nExemple avec 2 nÅ“uds masters\nmaster1  et master2\nHighly Available Kubernetes Cluster\n3\n\nQuorum = 2\nSi 1 tombe, il ne reste quâ€™un seul nÅ“ud â†’ quorum non atteint âŒ\nâ¡ï¸ Cluster bloquÃ© mÃªme sâ€™il te reste un master\nExemple avec 3 nÅ“uds masters\nmaster1 , master2 , master3\nQuorum = 2\nSi 1 tombe (peu importe lequel), il reste 2 nÅ“uds â†’ quorum ok âœ…\nâ¡ï¸ Cluster continue Ã  fonctionner normalement\nğŸŒ Câ€™est quoi lâ€™Ingress ?\nLâ€™Ingress est comme un portier dâ€™hÃ´tel ğŸ›ï¸. Il gÃ¨re les routes HTTP (ex: /admin , \n/app ) et les redirige vers les bonnes applis.\nğŸ§  Exemple :\nwww.monsite.com/admin   â†’ Pod A\nwww.monsite.com/shop    â†’ Pod B\nğŸ‘¨â€ğŸ’» Comment un utilisateur accÃ¨de Ã  lâ€™application ?\n1. Il tape une URL (ex : myapp.com )\n2. Le load balancer externe redirige vers lâ€™Ingress\n3. Lâ€™Ingress envoie la requÃªte vers le bon pod (ex: application Joget)\n4. Le pod rÃ©pond â†’ l'utilisateur voit son interface\nâš™ï¸ Aider un DevOps Ã  configurer les VM (pour Joget + DB)\nVoici une proposition simple et rÃ©aliste :\nRÃ´le VM\nCPU\nRAM\nStockage\nDÃ©tails\nMaster Node\n(x3)\n2\n2 Go\n40 Go\netcd + control\nplane\nHighly Available Kubernetes Cluster\n4\n\nWorker Node\n(x2+)\n4\n8 Go\n60 Go\nJoget + apps\nBase de\ndonnÃ©es (DB)\n4\n8 Go\n60 Go\nMySQL/Postgres\nLoad Balancer\n(x2)\n1\n1 Go\n20 Go\nHAProxy +\nKeepalived\nNFS Server\n(option)\n1\n2 Go\n100 Go\nStockage partagÃ©\nğŸ”¢ Les 3 Load Balancers dans ton cluster Kubernetes \nHA :\n1. ğŸŒ Load Balancer Externe\nRÃ´le : Point d'entrÃ©e pour les utilisateurs finaux (navigateur, mobile, API, etc.)\nIP publique/virtuelle : ex. 192.168.1.100  ou myapp.company.com\nRedirige vers : les 2 load balancers internes (HAProxy)\nSouvent gÃ©rÃ© par : Keepalived (pour failover) + floating IP\n2. ğŸ” Load Balancer Interne 1 (HAProxy 1) et Interne 2 (HAProxy \n2)\nRÃ´le : rÃ©partissent le trafic vers les kube-apiserver  des 3 nÅ“uds de contrÃ´le\nIls sont en redondance grÃ¢ce Ã  Keepalived\nIls assurent que le plan de contrÃ´le reste disponible mÃªme si un LB interne \ntombe\nğŸ“Œ Comment fonctionne un cluster Kubernetes avec plusieurs \nmasters ?\nDans une architecture multi-masters, plusieurs serveurs (nÅ“uds de contrÃ´le) \nhÃ©bergent chacun une copie des composants critiques de Kubernetes ( kube-\napiserver , etcd , scheduler , etc.).\nHighly Available Kubernetes Cluster\n5\n\nğŸ‘‰ Cela permet au cluster de continuer Ã  fonctionner mÃªme si un ou deux masters \ntombent en panne. Câ€™est le principe de la haute disponibilitÃ© (HA).\nâš–ï¸ Pourquoi faut-il un Load Balancer interne ?\nLe Load Balancer interne agit comme un point dâ€™entrÃ©e unique vers les kube-\napiserver  rÃ©partis sur les masters.\nTous les composants (kubelet, kubectl, etc.) communiquent avec cette seule \nadresse (ex: 192.168.193.135:6443 ), sans se soucier de quel master est actif.\nâœ… Ses avantages :\nğŸ” RÃ©partit le trafic entre les masters\nğŸ’¥ Si un master tombe, les autres prennent le relais automatiquement\nğŸ§© Simplifie la configuration : une seule IP pour accÃ©der au cluster\nâŒ Et si on nâ€™en met pas ?\nSans Load Balancer :\nTu dois taper manuellement sur les IPs des masters\nPas de bascule automatique en cas de panne\nLe cluster devient fragile et perd le bÃ©nÃ©fice de la haute dispo\nHighly Available Kubernetes Cluster\n6\n\n"
}