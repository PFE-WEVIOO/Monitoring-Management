{
  "filename": "Cluster K8S HA.pdf",
  "content": "Cluster kubernetes HA\nVagrantFile\nVAGRANT_NODES = [\n  { :hostname => \"k8s-master1\", :ip => \"192.168.56.11\" },\n  { :hostname => \"k8s-master2\", :ip => \"192.168.56.12\" },\n  { :hostname => \"k8s-master3\", :ip => \"192.168.56.13\" },\n  { :hostname => \"k8s-lb1\",     :ip => \"192.168.56.14\" },\n  { :hostname => \"k8s-lb2\",     :ip => \"192.168.56.15\" },\n  { :hostname => \"k8s-nfs\",     :ip => \"192.168.56.16\" },\n  { :hostname => \"k8s-worker1\", :ip => \"192.168.56.17\" }\n]\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"ubuntu/jammy64\"\n  config.vm.boot_timeout = 600\n  VAGRANT_NODES.each do |node|\n    config.vm.define node[:hostname] do |node_config|\n      node_config.vm.hostname = node[:hostname]\n      node_config.vm.network \"private_network\", ip: node[:ip]\n      node_config.vm.provider \"virtualbox\" do |vb|\n        if node[:hostname].start_with?(\"k8s-lb\", \"k8s-nfs\")\n          vb.memory = 512\n          vb.cpus = 1\n        elsif node[:hostname].start_with?(\"k8s-master\")\n          vb.memory = 4096\n          vb.cpus = 2\n        else\n          vb.memory = 2048\n          vb.cpus = 2\n        end\nCluster kubernetes HA\n1\n\n        vb.gui = false\n      end\n    end\n  end\nend\nNom de la VM\nIP\nRôle\nRAM\nvCPU\nk8s-master1\n192.168.56.11\nMaster node\n4096 MB\n2\nk8s-master2\n192.168.56.12\nMaster node\n4096 MB\n2\nk8s-master3\n192.168.56.13\nMaster node\n4096 MB\n2\nk8s-lb1\n192.168.56.14\nLoad balancer\n512 MB\n1\nk8s-lb2\n192.168.56.15\nLoad balancer\n512 MB\n1\nk8s-nfs\n192.168.56.16\nNFS Server\n512 MB\n1\nk8s-worker1\n192.168.56.17\nWorker node\n2048 MB\n2\nChaque machine a un nom d’hôte et une adresse IP privée dans le réseau \n192.168.56.x .\n✅ Commandes nécessaires :\n1. Créer et démarrer toutes les VMs :\nvagrant up\n2. Accéder à une VM (ex. master1) :\nvagrant ssh k8s-master1\n3. Voir l’état de toutes les VMs :\nvagrant status\n4. Arrêter toutes les VMs :\nCluster kubernetes HA\n2\n\nvagrant halt\n5. Supprimer toutes les VMs (définitivement avec leurs disques) :\nvagrant destroy -f\nVoici une documentation claire et structurée de toutes les étapes nécessaires \npour préparer les nœuds k8s-master1  et k8s-worker1  à rejoindre un cluster \nKubernetes avec containerd  comme runtime :\nDéploiement Manuel d’un Cluster Kubernetes (v1.28)\n🧱 Désactiver le Firewall (temporairement)\nPermet de ne pas bloquer les communications entre les nœuds durant \nl’installation.\n```bash\nsudo ufw disable\n```\n🔄 Mise à jour du système & synchronisation de \nl’horloge\nKubernetes exige une synchronisation précise de l’heure entre les nœuds pour \nfonctionner correctement.\n```bash\nsudo apt update && sudo apt full-upgrade -y\n```\n```bash\nsudo apt install -y systemd-timesyncd\n```\n```bash\nsudo timedatectl set-ntp true\n```\n📴 Désactiver le Swap\nKubernetes n'autorise pas le swap. Il faut le désactiver temporairement et \ndéfinitivement.\nCluster kubernetes HA\n3\n\n# Désactiver temporairement :\n```bash\nsudo swapoff -a\n```\n# Désactiver définitivement :\n```bash\nsudo sed -i.bak -r 's/(.+ swap .+)/#\\1/' /etc/fstab\n```\n🧩 Activer les modules kernel requis\nCes modules permettent à Kubernetes de gérer correctement le réseau entre \npods.\necho -e \"overlay\\nbr_netfilter\" | sudo tee /etc/modules-load.d/k8s.conf\n```bash\nsudo modprobe overlay\n```\n```bash\nsudo modprobe br_netfilter\n```\n🌐 Configurer les paramètres réseau\nCes options permettent à iptables  de bien gérer le trafic réseau inter-pods.\ncat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n```bash\nsudo sysctl --system\n```\n📦 Installer les paquets nécessaires\nPréparer le système pour ajouter des dépôts tiers (Kubernetes, Docker…).\n```bash\nsudo apt install -y apt-transport-https ca-certificates curl gpg software-prope\n```\nCluster kubernetes HA\n4\n\nrties-common\nConfiguration du Load Balancer (k8s-lb1)\n🎯 Objectif :\nOffrir une IP virtuelle stable ( 192.168.56.14 ) exposée aux workers et aux masters \npour l’API Kubernetes.\nRépartir la charge entre k8s-master1 , k8s-master2 , k8s-master3 .\n📦 Installer HAProxy\n```bash\nsudo apt update\n```\n```bash\nsudo apt install -y haproxy\n```\n⚙️ Configurer /etc/haproxy/haproxy.cfg\nAjoute en bas du fichier la section suivante :\nfrontend kubernetes-api\n    bind *:6443\n    mode tcp\n    option tcplog\n    default_backend kubernetes-masters\nbackend kubernetes-masters\n    mode tcp\n    balance roundrobin\n    option tcp-check\n    default-server inter 3s fall 3 rise 2\n    server master1 192.168.56.11:6443 check\n    server master2 192.168.56.12:6443 check\n    server master3 192.168.56.13:6443 check\nCluster kubernetes HA\n5\n\n🚀 Redémarrer HAProxy :\n```bash\nsudo systemctl restart haproxy\n```\n```bash\nsudo systemctl enable haproxy\n```\n☸️ Installer Kubernetes (kubelet, kubeadm, kubectl)\n➤ Ajouter la clé GPG et le dépôt Kubernetes :\n```bash\nsudo mkdir -p /etc/apt/keyrings\n```\n```bash\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | \\\n```\n  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://\npkgs.k8s.io/core:/stable:/v1.28/deb/ /' | \\\n  sudo tee /etc/apt/sources.list.d/kubernetes.list\n➤ Installer les binaires Kubernetes :\n```bash\nsudo apt update\n```\n```bash\nsudo apt install -y kubelet kubeadm kubectl\n```\n```bash\nsudo apt-mark hold kubelet kubeadm kubectl\n```\nkubeadm : pour initialiser ou rejoindre un cluster\nkubelet  : agent sur chaque nœud\n```bash\nkubectl  : outil en ligne de commande pour gérer le cluster\n```\n🐳 Installer containerd (container runtime)\nKubernetes a besoin d’un runtime pour exécuter les conteneurs (ici, containerd ).\nCluster kubernetes HA\n6\n\n➤ Ajouter le dépôt Docker :\n```bash\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | \\\n```\n  sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docke\nr.gpg] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n➤ Installer containerd :\n```bash\nsudo apt update\n```\n```bash\nsudo apt install -y containerd.io\n```\n➤ Générer et modifier la configuration pour utiliser \nSystemdCgroup  :\n```bash\nsudo mkdir -p /etc/containerd\n```\n```bash\nsudo containerd config default | sudo tee /etc/containerd/config.toml > /dev/n\n```\null\n```bash\nsudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/container\n```\nd/config.toml\n➤ Redémarrer containerd :\n```bash\nsudo systemctl restart containerd\n```\n```bash\nsudo systemctl enable containerd\n```\n🛠️ Configurer crictl (outil de debug pour containerd)\ncrictl  est utile pour déboguer les conteneurs sans passer par Docker.\nCluster kubernetes HA\n7\n\n```bash\nsudo apt install -y cri-tools\n```\ncat <<EOF | sudo tee /etc/crictl.\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 2\ndebug: false\npull-image-on-create: false\nEOF\nSur chaque nœud (master1, master2, master3, worker1), éditer :\n```bash\nsudo nano /etc/default/kubelet\n```\nAjouter :\nKUBELET_EXTRA_ARGS=--node-ip=192.168.56.11  # changer l'IP selon le nœu\nd\nPuis recharger le service :\n```bash\nsudo systemctl daemon-reexec\n```\n```bash\nsudo systemctl restart kubelet\n```\n📥 Télécharger les images Kubernetes\nSur tous les nœuds / LoadBalancer :\nkubeadm config images pull --cri-socket unix:///run/containerd/containerd.so\nck\n🧠 Initialisation du Master1\nCluster kubernetes HA\n8\n\nSur k8s-master1  :\nkubeadm init \\\n  --apiserver-advertise-address=192.168.56.11 \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --control-plane-endpoint=192.168.56.14:6443 \\\n  --upload-certs \\\n  --cri-socket unix:///run/containerd/containerd.sock\n🔑 À noter après l’exécution :\nLe token pour rejoindre les nœuds\nLe hash -discovery-token-ca-cert-hash\nLa clé -certificate-key  pour les masters\n🧩 Configuration de kubectl  sur master1\nmkdir -p $HOME/.kube\n```bash\nsudo cp /etc/kubernetes/admin.conf $HOME/.kube/config\n```\n```bash\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n🌐 Installation du réseau CNI (Weave Net)\nSur master1  :\n```bash\nkubectl apply -f https://github.com/weaveworks/weave/releases/download/v\n```\n2.8.1/weave-daemonset-k8s.\n🧭 Ajouter les autres masters au cluster\nSur k8s-master2  (192.168.56.12) :\nCluster kubernetes HA\n9\n\nkubeadm join 192.168.56.14:6443 \\\n  --token <YOUR_TOKEN> \\\n  --discovery-token-ca-cert-hash sha256:<HASH> \\\n  --control-plane \\\n  --certificate-key <CERT_KEY> \\\n  --apiserver-advertise-address=192.168.56.12 \\\n  --cri-socket unix:///run/containerd/containerd.sock\nSur k8s-master3  (192.168.56.13) :\nkubeadm join 192.168.56.14:6443 \\\n  --token <YOUR_TOKEN> \\\n  --discovery-token-ca-cert-hash sha256:<HASH> \\\n  --control-plane \\\n  --certificate-key <CERT_KEY> \\\n  --apiserver-advertise-address=192.168.56.13 \\\n  --cri-socket unix:///run/containerd/containerd.sock\n👷 Ajouter le worker\nSur k8s-worker1  (192.168.56.17) :\nkubeadm join 192.168.56.14:6443 \\\n  --token <YOUR_TOKEN> \\\n  --discovery-token-ca-cert-hash sha256:<HASH> \\\n  --cri-socket unix:///run/containerd/containerd.sock\n✅ Vérification Finale sur Master1\n```bash\nkubectl get nodes -o wide\n```\n```bash\nkubectl get pods -A -o wide\n```\nCluster kubernetes HA\n10\n\nDocumentation – Automatisation de \nl’installation d’un cluster Kubernetes avec \nAnsible\n📂 Structure Générale\nk8s-cluster-ansible/\n├── ansible.cfg\n├── group_vars/\n│   └── all.yml\n├── inventory.yml\n├── keys/\n├── playbooks/\n├── roles/\n🧩 1. ansible.cfg\nCe fichier configure le comportement global d’Ansible.\n[defaults]\ninventory = inventory.yml     # Fichier d’inventaire personnalisé\nremote_user = root            # Utilisateur par défaut (souvent utilisé en root via s\nudo)\nhost_key_checking = False     # Désactive la vérification du fingerprint SSH\nroles_path = ./roles          # Chemin vers les rôles\n🟡 Remarque : Ce fichier est global mais est surpassé par les valeurs définies \ndans inventory.yml  (comme ansible_user  et ansible_ssh_private_key_file ).\n🗂️\nCluster kubernetes HA\n11\n\n🗂️ 2. group_vars/all.yml\nCe fichier contient les variables globales à tous les hôtes.\nk8s_version: \"1.28.15\"                           # Version de Kubernetes à installer\nk8s_apt_key: https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key\nk8s_apt_repo: \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gp\ng] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\"\ndocker_apt_key: https://download.docker.com/linux/ubuntu/gpg\ndocker_apt_repo: \"deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gp\ng] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release | l\nower }} stable\"\n📌 Utilisées dans les rôles comme containerd  et kubernetes  pour ajouter les dépôts.\n📋 3. inventory.yml\nC’est le fichier d’inventaire statique, qui déclare tous les nœuds avec leurs \nadresses IP, utilisateurs SSH et chemins de clés privées.\nall:\n  vars:\n    ansible_user: vagrant\n    ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key\n  children:\n    masters:\n      hosts:\n        k8s-master1:\n          ansible_host: 192.168.56.11\n          ansible_user: vagrant\n          ansible_ssh_private_key_file: keys/master1\n          kubeadm_token: ...\n          discovery_hash: ...\n          certificate_key: ...\nCluster kubernetes HA\n12\n\n        k8s-master2:\n          ...\n        k8s-master3:\n          ...\n    workers:\n      hosts:\n        k8s-worker1:\n          ...\n    loadbalancers:\n      hosts:\n        k8s-lb1:\n          ...\n📌 Les variables spécifiques ( token , hash , cert_key ) sont ajoutées uniquement sur \nk8s-master1  car c’est lui qui initie le cluster.\n🔐 4. keys/\nCe dossier contient les clés privées SSH Vagrant copiées depuis :\nC:\\vagrant-k8s-cluster\\.vagrant\\machines\\<vm>\\virtualbox\\private_key\n🔐 Ces clés permettent à Ansible d’accéder à chaque VM sans mot de passe.\n🗂️ 5. playbooks/\nCe dossier contiendra les playbooks Ansible pour automatiser chaque étape du \ndéploiement :\nsetup_cluster.yml  : préparation commune (disable swap, modules kernel, etc.)\nloadbalancer.yml  : configuration de HAProxy sur k8s-lb1\ninit_master.yml  : initialisation du premier master\njoin_master.yml  / join_worker.yml  : ajout des nœuds au cluster\ndeploy_network.yml  : installation du CNI (ex. Weave Net)\nCluster kubernetes HA\n13\n\ndeploy_cluster.yml  : exécution de tous les playbooks en une seule commande\nconfigure_crictl.yml  : configuration de crictl\nreset_cluster.yml  : suppression complète du cluster (utile en dev)\n📁 6. roles/\nRépertoire contenant les rôles Ansible pour chaque composant :\nRôle\nFonction principale\ncommon\nMise à jour système, désactivation swap, modules kernel, etc.\ncontainerd\nInstallation et configuration du runtime containerd\nkubernetes\nInstallation de kubelet, kubeadm, kubectl + sources APT\ninit_master\nInitialise k8s-master1 , génère la commande kubeadm join\njoin_master\nFait rejoindre k8s-master2/3  au cluster en mode control-plane\njoin_worker\nFait rejoindre les workers\nloadbalancer\nInstalle et configure haproxy  (via haproxy.cfg.j2 )\nreset_cluster\nRéinitialise complètement les nœuds Kubernetes\n🧩 Rôle common\n📍 Cible : Tous les nœuds (masters, workers, loadbalancer)\n🔧 Objectif : Appliquer toutes les prérequis système communs à Kubernetes.\n🔧 Tâches exécutées :\nOrdre\nTâche\nDescription\n1\nDésactiver le firewall\n(ufw)\nÉvite les blocages de ports entre les nœuds\n2\nMise à jour complète\nMise à jour du cache et des paquets ( apt\nfull-upgrade )\n3\nInstaller systemd-\ntimesyncd\nSynchronisation automatique de l’horloge\nCluster kubernetes HA\n14\n\n4\nActiver NTP\nActive la synchronisation de l'heure\n5\nDésactiver le swap\nRequis par Kubernetes (temporaire +\npermanent via /etc/fstab )\n6\nCharger les modules\nkernel requis\noverlay  et br_netfilter\n7\nConfigurer sysctl\nActive le forwarding IP et règles de pont\nréseau\n8\nInstaller les paquets de\nbase\ncurl, gpg, etc.\n✅ Résultat attendu : Le système est prêt pour Kubernetes : pas de swap, réseau \nactivé, modules chargés, heure à jour.\n🐳 Rôle containerd\n📍 Cible : Masters & Workers uniquement\n🔧 Objectif : Installer et configurer containerd  (le runtime conteneur).\n🔧 Tâches exécutées :\nOrdre\nTâche\nDescription\n1\nAjouter la clé GPG Docker\nSécurise l’accès au dépôt\n2\nAjouter le dépôt Docker\nPermet l’installation de containerd  depuis\nDocker\n3\nInstaller containerd\nLe runtime container utilisé par\nKubernetes\n4\nGénérer le fichier\nconfig.toml\nAvec containerd config default\n5\nConfigurer SystemdCgroup =\ntrue\nRequis pour compatibilité avec kubelet\n6\nRedémarrer et activer\ncontainerd\nAppliquer la config dès le boot\n✅ Résultat attendu : containerd  fonctionne avec les bons paramètres pour \nKubernetes ( SystemdCgroup  activé).\n☸️\nCluster kubernetes HA\n15\n\n☸️ Rôle kubernetes\n📍 Cible : Masters & Workers uniquement\n🔧 Objectif : Installer les outils de base Kubernetes : kubelet , kubeadm , kubectl .\n🔧 Tâches exécutées :\nOrdre\nTâche\nDescription\n1\nCréer /etc/apt/keyrings\nDossier pour stocker les clés\n2\nTélécharger la clé du repo\nKubernetes\nSignature de confiance\n3\nAjouter le dépôt APT de\nKubernetes\nAccès à kubeadm , kubelet , etc.\n4\nMettre à jour les paquets APT\nIntègre le nouveau dépôt\n5\nInstaller kubelet , kubeadm ,\nkubectl\nLes outils essentiels\n6\nHold  des versions\nEmpêche leur mise à jour automatique\n(stabilité)\n✅ Résultat attendu : Kubernetes est installé et prêt à être initialisé ou rejoint via \nkubeadm .\n🧪 Playbooks de test\nCes playbooks permettent d’exécuter chaque rôle séparément pour s’assurer \nqu’ils fonctionnent bien.\n▶️ test_common.yml\n- name: Test common role\n  hosts: all\n  become: yes\n  roles:\n    - role: common\n      tags: common\nCluster kubernetes HA\n16\n\n➡️ Applique le rôle common  à tous les nœuds.\n▶️ test_containerd.yml\n- name: Test containerd role\n  hosts: masters,workers\n  become: true\n  tags: containerd\n  roles:\n    - containerd\n➡️ Applique le rôle containerd  uniquement aux masters et workers.\n▶️ test_kubernetes.yml\n- name: Test kubernetes role\n  hosts: masters,workers\n  become: true\n  tags: kubernetes\n  roles:\n    - kubernetes\n➡️ Applique le rôle kubernetes  uniquement aux masters et workers.\n🧩 Rôle loadbalancer\n📍 Cible : k8s-lb1\n🎯 Objectif : Installer et configurer HAProxy pour équilibrer la charge sur les \nmasters.\n📄 Fichier haproxy.cfg.j2\nContient la configuration HAProxy pour écouter sur le port 6443  et répartir les \nrequêtes vers les 3 masters :\nCluster kubernetes HA\n17\n\nfrontend kubernetes-frontend\n    bind *:6443\n    mode tcp\n    option tcplog\n    default_backend kubernetes-backend\nbackend kubernetes-backend\n    mode tcp\n    balance roundrobin\n    option tcp-check\n    default-server inter 10s downinter 5s rise 2 fall 3\n    server k8s-master1 192.168.56.11:6443 check\n    server k8s-master2 192.168.56.12:6443 check\n    server k8s-master3 192.168.56.13:6443 check\n📜 Tâches loadbalancer/tasks/main.yml\n1. Installe HAProxy\n2. Déploie le fichier de config via un template Jinja\n3. Redémarre et active le service\n🚀 Rôle init_master\n📍 Cible : k8s-master1\n🎯 Objectif : Initialiser le cluster Kubernetes.\n🔧 Étapes :\n1. Tirer les images nécessaires avec kubeadm config images pull\n2. Lancer kubeadm init  avec :\nIP de l’API : 192.168.56.14  (Load Balancer)\nCIDR du réseau Pod : 10.244.0.0/16  (Weave)\nCluster kubernetes HA\n18\n\nActivation du certificat partagé -upload-certs\n3. Sauvegarde la sortie dans /root/kubeadm-init-output.txt\n4. Configure kubectl  pour root ( ~/.kube/config )\n5. Génère un script gen_join_cmd.sh  via le template Jinja :\nExtrait et nettoie les commandes kubeadm join  du fichier texte\nGénére /root/join-master.sh  automatiquement exécutable\n🔁 Rôle join_master\n📍 Cible : k8s-master2  & k8s-master3\n🎯 Objectif : Faire rejoindre les autres masters au cluster en mode control-plane.\n🔧 Étapes :\n1. Exécute kubeadm join  avec :\nToken et hash fournis depuis k8s-master1\nClé du certificat ( -certificate-key )\n2. Configure kubectl  ( ~/.kube/config ) pour debug/commande locale\n🧱 Rôle join_worker\n📍 Cible : k8s-worker1\n🎯 Objectif : Ajouter les nœuds workers au cluster.\n🔧 Étapes :\n1. Exécute kubeadm join  avec :\nToken et hash depuis k8s-master1\nIP publique du load balancer\n2. Ajoute kubelet.conf  dans ~/.kube/config  pour debug local\n🛠️\nCluster kubernetes HA\n19\n\n🛠️ Playbook configure_crictl.yml\n📍 Cible : masters & workers\n🎯 Objectif : Installer et configurer crictl  (outil pour interagir avec containerd).\n🔧 Étapes :\n1. Installe cri-tools\n2. Crée /etc/crictl.  avec les bons chemins socket pour containerd\n🌐 Playbook deploy_network.yml\n📍 Cible : k8s-master1\n🎯 Objectif : Installer le plugin réseau Weave Net (CNI)\n🔧 Étape unique :\nApplique le manifest officiel Weave Net :\n```bash\nkubectl apply -f https://github.com/weaveworks/weave/releases/download/v\n```\n2.8.1/weave-dae\n📘 Extrait de la documentation\n# 🚀 Kubernetes HA Cluster avec Ansible\nCe projet permet de déployer automatiquement un cluster Kubernetes haute d\nisponibilité (multi-master) avec un Load Balancer, `containerd` comme runtim\ne, et Weave Net comme plugin réseau.\n📦 Technologies utilisées :\n- Ansible\n- Kubernetes v1.28\n- containerd\n- HAProxy (load balancing)\n- Weave Net (CNI)\nCluster kubernetes HA\n20\n\n## 📥 Cloner ce dépôt\n```\ngit clone https://github.com/motrabelsi10/k8s-ansible-cluster.git\ncd k8s-ansible-cluster\n🧪 Structure du projet\ninventory.yml  : inventaire des nœuds\ngroup_vars/all.yml  : variables globales (versions, dépôts)\nplaybooks/  : scripts Ansible organisés par étape\nroles/  : tâches organisées par composant (common, containerd, kubernetes, \netc.)\n▶️ Déploiement complet\nPour exécuter tout le déploiement automatiquement :\nansible-playbook playbooks/deploy_cluster.yml\n⚠️ Assure-toi que :\nLes clés SSH dans keys/  sont correctes\nLes VMs sont accessibles depuis la machine d'exécution\nAnsible est installé sur ta machine\nCluster kubernetes HA\n21\n\n🚀 Ansible Kubernetes Helm Deployment — \nMySQL & Joget\nCe projet permet d'automatiser le déploiement de MySQL puis Joget sur un \ncluster Kubernetes grâce à Helm et Ansible, en respectant les bonnes pratiques \n(rôles, variables, tags, inventaire, etc.).\n📁 Arborescence du projet\nansible-joget-deploy/\n├── ansible.cfg                 # Configuration Ansible\n├── inventory.yml              # Inventaire des hôtes\n├── group_vars/\n│   └── all.yml                # Variables globales\n├── charts/                    # (Optionnel) Chart local\n│   └── mysql/                 # Chart Helm MySQL\n├── playbooks/\n│   ├── deploy_mysql.yml       # Déploie uniquement MySQL\n│   ├── deploy_joget.yml       # Déploie uniquement Joget\n│   └── deploy_all.yml         # Déploie MySQL puis Joget\n└── roles/\n    ├── check_helm/            # Vérifie que Helm est installé\n    │   └── tasks/main.yml\n    ├── mysql/                 # Déploiement Helm de MySQL\n    │   └── tasks/main.yml\n    └── joget/                 # Déploiement Helm de Joget\n        └── tasks/main.yml\n🔧 Pré-requis\nCluster Kubernetes opérationnel (avec un nœud k8s-worker1 )\nCluster kubernetes HA\n22\n\nHelm installé uniquement sur k8s-master1\nSSH sans mot de passe fonctionnel avec Ansible\nLes charts Helm mysql  et joget  disponibles dans :\n/home/vagrant/joget-helm/mysql\n/home/vagrant/joget-helm/joget\n📄 Fichier group_vars/all.yml\nnamespace: joget\nmysql_chart_path: /home/vagrant/joget-helm/mysql\njoget_chart_path: /home/vagrant/joget-helm/joget\nrelease_name_mysql: mysql\nrelease_name_joget: joget\n📄 Fichier inventory.yml\nall:\n  vars:\n    ansible_user: vagrant\n    ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key\n  children:\n    masters:\n      hosts:\n        k8s-master1:\n          ansible_host: 192.168.56.11\n          ansible_user: vagrant\n          ansible_ssh_private_key_file: /home/vagrant/k8s-cluster-ansible/keys/m\naster1\n📄\nCluster kubernetes HA\n23\n\n📄 Fichier ansible.cfg\n[defaults]\ninventory = inventory.yml\nremote_user = root\nhost_key_checking = False\nroles_path = ./roles\n📜 Rôle check_helm/tasks/main.yml\nVérifie si Helm est installé, sinon échoue.\n- name: Check if helm is installed\n  command: helm version\n  register: helm_check\n  ignore_errors: true\n- name: Fail if helm is not installed\n  fail:\n    msg: \"Helm n'est pas installé. Veuillez l’installer.\"\n  when: helm_check.rc != 0\n📜 Rôle mysql/tasks/main.yml\nDéploie MySQL dans le namespace joget  :\n- name: Deploy MySQL with Helm\n  command: >\n    helm upgrade --install {{ release_name_mysql }}\n    {{ mysql_chart_path }}\n    -n {{ namespace }} --create-namespace\n  tags: mysql\nCluster kubernetes HA\n24\n\nLe chart Helm cible spécifiquement k8s-worker1 via un \nnodeSelector dans values..\n📜 Rôle joget/tasks/main.yml\nDéploie Joget dans le même namespace :\n- name: Deploy Joget with Helm\n  command: >\n    helm upgrade --install {{ release_name_joget }}\n    {{ joget_chart_path }}\n    -n {{ namespace }}\n  tags: joget\n▶️ Playbook deploy_mysql.yml\n- name: Check Helm and Deploy MySQL\n  hosts: k8s-master1\n  become: true\n  roles:\n    - check_helm\n    - mysql\n▶️ Playbook deploy_joget.yml\n- name: Deploy Joget Application\n  hosts: k8s-master1\n  become: true\n  roles:\n    - check_helm\n    - joget\n▶️\nCluster kubernetes HA\n25\n\n▶️ Playbook deploy_all.yml\nDéploie MySQL puis Joget dans le bon ordre :\n- name: Deploy MySQL then Joget\n  hosts: k8s-master1\n  become: true\n  roles:\n    - check_helm\n    - mysql\n    - joget\n🧪 Exécution des playbooks\nDéployer uniquement MySQL :\nansible-playbook playbooks/deploy_mysql.yml\nDéployer uniquement Joget :\nansible-playbook playbooks/deploy_joget.yml\nDéployer tout (MySQL puis Joget) :\nansible-playbook playbooks/deploy_all.yml\n✅ Vérification\nAprès exécution, tu peux vérifier les pods :\n```bash\nkubectl get pods -n joget -o wide\n```\nCluster kubernetes HA\n26\n\nLes pods mysql  et joget  doivent être en statut Running , et tourner sur k8s-worker1  si \nton nodeSelector  est bien configuré.\n🎯 Rôle du VIP (Virtual IP)\nLe VIP agit comme un point d’accès unique vers le cluster Kubernetes, en \nparticulier vers les API Servers des master  nodes.\n🔁 Pourquoi c’est important ?\n1. Abstraction du backend\nTu n’as pas besoin de connaître l’IP réelle des masters ( 192.168.56.11 , .12 , .13 ).\nTu utilises une seule IP (VIP), par exemple 192.168.56.30 .\n2. Équilibrage de charge\nLe VIP est associé à HAProxy sur les load balancers ( k8s-lb1 , k8s-lb2 ).\nLe trafic est réparti automatiquement entre les différents masters.\n3. Haute disponibilité\nSi k8s-lb1  tombe, Keepalived bascule le VIP vers k8s-lb2 .\nLe cluster continue de répondre via la même IP, sans interruption.\n🧠 Exemple concret\nDans ta commande kubeadm init  :\nkubeadm init \\\n  --control-plane-endpoint=192.168.56.30:6443 \\\n  ...\nCluster kubernetes HA\n27\n\n192.168.56.30  est le VIP\nIl pointe vers les 2 LBs (grâce à Keepalived)\nEt les LBs redirigent vers les masters (grâce à HAProxy)\n✅ 1. Objectif\nMettre en place deux load balancers avec :\nHAProxy : pour répartir la charge entre les 3 masters.\nKeepalived : pour fournir une IP virtuelle (VIP) en cas de panne de l’un des LB.\n📁 2. Architecture\nVIP (ex: 192.168.56.30)\n   |\n   |-- k8s-lb1 (192.168.56.14) -- HAProxy\n   |-- k8s-lb2 (192.168.56.15) -- HAProxy\n        \\__ Cluster Masters: 192.168.56.11, .12, .13\n⚙️ 3. Étapes pour chaque LB\n🧩 3.1. Installer HAProxy et Keepalived\nCréer un rôle Ansible loadbalancer  (ou configurer manuellement) :\n```bash\nsudo apt update && sudo apt install -y haproxy keepalived\n```\n📝 3.2. Configurer HAProxy /etc/haproxy/haproxy.cfg  :\nglobal\n  log /dev/log local0\n  maxconn 2000\n  daemon\nCluster kubernetes HA\n28\n\ndefaults\n  log global\n  mode tcp\n  option tcplog\n  timeout connect 10s\n  timeout client 1m\n  timeout server 1m\nfrontend kubernetes\n  bind *:6443\n  default_backend kubernetes-backend\nbackend kubernetes-backend\n  option httpchk GET /healthz\n  http-check expect status 200\n  server master1 192.168.56.11:6443 check\n  server master2 192.168.56.12:6443 check\n  server master3 192.168.56.13:6443 check\nRedémarrer HAProxy :\n```bash\nsudo systemctl restart haproxy\n```\n🛡️ 3.3. Configurer Keepalived /etc/keepalived/keepalived.conf\nSur k8s-lb1  (MASTER) :\nvrrp_instance VI_1 {\n    state MASTER\n    interface enp0s8\n    virtual_router_id 51\n    priority 101\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass secret\nCluster kubernetes HA\n29\n\n    }\n    virtual_ipaddress {\n        192.168.56.30\n    }\n}\nSur k8s-lb2  (BACKUP) :\nvrrp_instance VI_1 {\n    state BACKUP\n    interface enp0s8\n    virtual_router_id 51\n    priority 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass secret\n    }\n    virtual_ipaddress {\n        192.168.56.30\n    }\n}\nRedémarrer Keepalived sur les deux :\n```bash\nsudo systemctl restart keepalived\n```\n✅ 4. Tester\nPinger le VIP :\nping 192.168.56.30\nCluster kubernetes HA\n30\n\nVérifier que le port 6443 est ouvert :\ntelnet 192.168.56.30 6443\nPour simuler un failover :\n```bash\nsudo systemctl stop keepalived  # sur k8s-lb1\n```\n📦 Documentation des Charts Helm – Joget \n& MySQL\n🔍 Qu’est-ce que Helm ?\nHelm est un gestionnaire de packages pour Kubernetes. Il fonctionne comme \napt  pour Ubuntu ou yum  pour CentOS, mais pour déployer et gérer des \napplications dans Kubernetes.\n✅ Avantages de Helm :\n📦 Regroupe tous les fichiers de déploiement dans un seul package appelé \nchart\n🔁 Permet des mises à jour versionnées via helm upgrade\n🛠️ Personnalisation facile via un fichier values.\n♻️ Réutilisable, portable, et facile à partager\n📁 Structure générale d’un chart Helm\nmy-chart/\n├── Chart.            # Métadonnées du chart\n├── values.           # Paramètres configurables\nCluster kubernetes HA\n31\n\n└── templates/            # Fichiers manifestes Kubernetes ( avec Go templates)\n    ├── deployment.\n    ├── service.\n    ├── pvc.\n    ├── pv.\n    ├── configmap.\n    └── _helpers.tpl       # Fonctions utilitaires (optionnel)\n🎯 Chart Helm – Joget\n📄 Chart.\n```yaml\napiVersion: v2\nname: joget\ndescription: A Helm chart for Joget\nversion: 0.1.0\nappVersion: \"8.0\"\nDéfinit le nom du chart, sa version et la version de l'application Joget.\n📄 values.\nreplicaCount: 1\nimage:\n  repository: motrabelsi10/joget-v7\n  tag: latest\n  pullPolicy: Always\nmysql:\n  host: mysql.joget.svc.cluster.local\n  database: jwdb\n  username: tomcat\n  password: tomcat\nCluster kubernetes HA\n32\n\n```\nservice:\n  type: NodePort\n  portHttp: 8080\n  portHttps: 9080\n  nodePortHttp: 32325\nnodeSelector:\n  kubernetes.io/hostname: k8s-worker1\nDéfinit :\nL’image Docker de Joget\nLa configuration de connexion MySQL\nLe type de service exposé ( NodePort )\nLa planification du pod sur k8s-worker1\n📄 templates/deployment.\nDéploie Joget avec :\nUn replica\nDes variables d’environnement pour la DB\nUn volume monté depuis une PVC\n📄 templates/service.\nExpose Joget avec :\nDeux ports : HTTP (8080) & HTTPS (9080)\nNodePort  fixé à 32325  (accès depuis l’extérieur)\nAffinité de session ( ClientIP )\n📄 templates/pv.\nCrée un volume persistant (PV) sur le worker  :\nCluster kubernetes HA\n33\n\nhostPath:\n  path: \"/mnt/data/joget\"\nAvec :\n2Gi de stockage\nReadWriteMany\nNode affinity sur k8s-worker1\n📄 templates/pvc.\nDemande un volume persistant de 2Gi lié au PV précédent.\n📄 templates/serviceaccount.\nUn ClusterRoleBinding  pour permettre à Joget d’accéder à certaines ressources \nKubernetes.\n🧰 Commandes Helm Essentielles\n📥 1. Installer un chart\nhelm install <release-name> <chart-path> -n <namespace> --create-namesp\nace\n🔁 Exemple :\nhelm install joget ./joget -n joget --create-namespace\nhelm install mysql ./mysql -n joget --create-namespace\n♻️\nCluster kubernetes HA\n34\n\n♻️ 2. Mettre à jour un chart déjà installé (upgrade)\nhelm upgrade <release-name> <chart-path> -n <namespace>\n🔁 Exemple :\nhelm upgrade joget ./joget -n joget\nhelm upgrade mysql ./mysql -n joget\n🗑️ 3. Supprimer un déploiement Helm\nhelm uninstall <release-name> -n <namespace>\n🔁 Exemple :\nhelm uninstall joget -n joget\nhelm uninstall mysql -n joget\n📄 4. Lister les releases installées\nhelm list -A\n📌 -A  montre toutes les releases dans tous les namespaces.\n🔍 5. Afficher les ressources installées par une \nrelease\nhelm get manifest <release-name> -n <namespace>\n🔁 Exemple :\nCluster kubernetes HA\n35\n\nhelm get manifest joget -n joget\n🧪 6. Tester un chart localement (dry-run)\nhelm install <release-name> <chart-path> --dry-run --debug -n <namespace\n>\n🔁 Exemple :\nhelm install joget ./joget --dry-run --debug -n joget\n📝 7. Afficher les valeurs par défaut d’un chart\nhelm show values <chart-path>\n🔧 8. Personnaliser les valeurs avec un fichier\nhelm upgrade --install <release-name> <chart-path> -f my-values.yaml -n <n\namespace>\nCluster kubernetes HA\n36\n\n"
}