{
  "filename": "Cluster K8S HA.pdf",
  "content": "Cluster kubernetes HA\nVagrantFile\nVAGRANT_NODES = [\n  { :hostname => \"k8s-master1\", :ip => \"192.168.56.11\" },\n  { :hostname => \"k8s-master2\", :ip => \"192.168.56.12\" },\n  { :hostname => \"k8s-master3\", :ip => \"192.168.56.13\" },\n  { :hostname => \"k8s-lb1\",     :ip => \"192.168.56.14\" },\n  { :hostname => \"k8s-lb2\",     :ip => \"192.168.56.15\" },\n  { :hostname => \"k8s-nfs\",     :ip => \"192.168.56.16\" },\n  { :hostname => \"k8s-worker1\", :ip => \"192.168.56.17\" }\n]\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"ubuntu/jammy64\"\n  config.vm.boot_timeout = 600\n  VAGRANT_NODES.each do |node|\n    config.vm.define node[:hostname] do |node_config|\n      node_config.vm.hostname = node[:hostname]\n      node_config.vm.network \"private_network\", ip: node[:ip]\n      node_config.vm.provider \"virtualbox\" do |vb|\n        if node[:hostname].start_with?(\"k8s-lb\", \"k8s-nfs\")\n          vb.memory = 512\n          vb.cpus = 1\n        elsif node[:hostname].start_with?(\"k8s-master\")\n          vb.memory = 4096\n          vb.cpus = 2\n        else\n          vb.memory = 2048\n          vb.cpus = 2\n        end\nCluster kubernetes HA\n1\n\n        vb.gui = false\n      end\n    end\n  end\nend\nNom de la VM\nIP\nRÃ´le\nRAM\nvCPU\nk8s-master1\n192.168.56.11\nMaster node\n4096 MB\n2\nk8s-master2\n192.168.56.12\nMaster node\n4096 MB\n2\nk8s-master3\n192.168.56.13\nMaster node\n4096 MB\n2\nk8s-lb1\n192.168.56.14\nLoad balancer\n512 MB\n1\nk8s-lb2\n192.168.56.15\nLoad balancer\n512 MB\n1\nk8s-nfs\n192.168.56.16\nNFS Server\n512 MB\n1\nk8s-worker1\n192.168.56.17\nWorker node\n2048 MB\n2\nChaque machine a un nom dâ€™hÃ´te et une adresse IP privÃ©e dans le rÃ©seau \n192.168.56.x .\nâœ… Commandes nÃ©cessaires :\n1. CrÃ©er et dÃ©marrer toutes les VMs :\nvagrant up\n2. AccÃ©der Ã  une VM (ex. master1) :\nvagrant ssh k8s-master1\n3. Voir lâ€™Ã©tat de toutes les VMs :\nvagrant status\n4. ArrÃªter toutes les VMs :\nCluster kubernetes HA\n2\n\nvagrant halt\n5. Supprimer toutes les VMs (dÃ©finitivement avec leurs disques) :\nvagrant destroy -f\nVoici une documentation claire et structurÃ©e de toutes les Ã©tapes nÃ©cessaires \npour prÃ©parer les nÅ“uds k8s-master1  et k8s-worker1  Ã  rejoindre un cluster \nKubernetes avec containerd  comme runtime :\nDÃ©ploiement Manuel dâ€™un Cluster Kubernetes (v1.28)\nğŸ§± DÃ©sactiver le Firewall (temporairement)\nPermet de ne pas bloquer les communications entre les nÅ“uds durant \nlâ€™installation.\n```bash\nsudo ufw disable\n```\nğŸ”„ Mise Ã  jour du systÃ¨me & synchronisation de \nlâ€™horloge\nKubernetes exige une synchronisation prÃ©cise de lâ€™heure entre les nÅ“uds pour \nfonctionner correctement.\n```bash\nsudo apt update && sudo apt full-upgrade -y\n```\n```bash\nsudo apt install -y systemd-timesyncd\n```\n```bash\nsudo timedatectl set-ntp true\n```\nğŸ“´ DÃ©sactiver le Swap\nKubernetes n'autorise pas le swap. Il faut le dÃ©sactiver temporairement et \ndÃ©finitivement.\nCluster kubernetes HA\n3\n\n# DÃ©sactiver temporairement :\n```bash\nsudo swapoff -a\n```\n# DÃ©sactiver dÃ©finitivement :\n```bash\nsudo sed -i.bak -r 's/(.+ swap .+)/#\\1/' /etc/fstab\n```\nğŸ§© Activer les modules kernel requis\nCes modules permettent Ã  Kubernetes de gÃ©rer correctement le rÃ©seau entre \npods.\necho -e \"overlay\\nbr_netfilter\" | sudo tee /etc/modules-load.d/k8s.conf\n```bash\nsudo modprobe overlay\n```\n```bash\nsudo modprobe br_netfilter\n```\nğŸŒ Configurer les paramÃ¨tres rÃ©seau\nCes options permettent Ã  iptables  de bien gÃ©rer le trafic rÃ©seau inter-pods.\ncat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n```bash\nsudo sysctl --system\n```\nğŸ“¦ Installer les paquets nÃ©cessaires\nPrÃ©parer le systÃ¨me pour ajouter des dÃ©pÃ´ts tiers (Kubernetes, Dockerâ€¦).\n```bash\nsudo apt install -y apt-transport-https ca-certificates curl gpg software-prope\n```\nCluster kubernetes HA\n4\n\nrties-common\nConfiguration du Load Balancer (k8s-lb1)\nğŸ¯ Objectif :\nOffrir une IP virtuelle stable ( 192.168.56.14 ) exposÃ©e aux workers et aux masters \npour lâ€™API Kubernetes.\nRÃ©partir la charge entre k8s-master1 , k8s-master2 , k8s-master3 .\nğŸ“¦ Installer HAProxy\n```bash\nsudo apt update\n```\n```bash\nsudo apt install -y haproxy\n```\nâš™ï¸ Configurer /etc/haproxy/haproxy.cfg\nAjoute en bas du fichier la section suivante :\nfrontend kubernetes-api\n    bind *:6443\n    mode tcp\n    option tcplog\n    default_backend kubernetes-masters\nbackend kubernetes-masters\n    mode tcp\n    balance roundrobin\n    option tcp-check\n    default-server inter 3s fall 3 rise 2\n    server master1 192.168.56.11:6443 check\n    server master2 192.168.56.12:6443 check\n    server master3 192.168.56.13:6443 check\nCluster kubernetes HA\n5\n\nğŸš€ RedÃ©marrer HAProxy :\n```bash\nsudo systemctl restart haproxy\n```\n```bash\nsudo systemctl enable haproxy\n```\nâ˜¸ï¸ Installer Kubernetes (kubelet, kubeadm, kubectl)\nâ¤ Ajouter la clÃ© GPG et le dÃ©pÃ´t Kubernetes :\n```bash\nsudo mkdir -p /etc/apt/keyrings\n```\n```bash\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | \\\n```\n  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://\npkgs.k8s.io/core:/stable:/v1.28/deb/ /' | \\\n  sudo tee /etc/apt/sources.list.d/kubernetes.list\nâ¤ Installer les binaires Kubernetes :\n```bash\nsudo apt update\n```\n```bash\nsudo apt install -y kubelet kubeadm kubectl\n```\n```bash\nsudo apt-mark hold kubelet kubeadm kubectl\n```\nkubeadm : pour initialiser ou rejoindre un cluster\nkubelet  : agent sur chaque nÅ“ud\n```bash\nkubectl  : outil en ligne de commande pour gÃ©rer le cluster\n```\nğŸ³ Installer containerd (container runtime)\nKubernetes a besoin dâ€™un runtime pour exÃ©cuter les conteneurs (ici, containerd ).\nCluster kubernetes HA\n6\n\nâ¤ Ajouter le dÃ©pÃ´t Docker :\n```bash\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | \\\n```\n  sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docke\nr.gpg] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\nâ¤ Installer containerd :\n```bash\nsudo apt update\n```\n```bash\nsudo apt install -y containerd.io\n```\nâ¤ GÃ©nÃ©rer et modifier la configuration pour utiliser \nSystemdCgroup  :\n```bash\nsudo mkdir -p /etc/containerd\n```\n```bash\nsudo containerd config default | sudo tee /etc/containerd/config.toml > /dev/n\n```\null\n```bash\nsudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/container\n```\nd/config.toml\nâ¤ RedÃ©marrer containerd :\n```bash\nsudo systemctl restart containerd\n```\n```bash\nsudo systemctl enable containerd\n```\nğŸ› ï¸ Configurer crictl (outil de debug pour containerd)\ncrictl  est utile pour dÃ©boguer les conteneurs sans passer par Docker.\nCluster kubernetes HA\n7\n\n```bash\nsudo apt install -y cri-tools\n```\ncat <<EOF | sudo tee /etc/crictl.\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 2\ndebug: false\npull-image-on-create: false\nEOF\nSur chaque nÅ“ud (master1, master2, master3, worker1), Ã©diter :\n```bash\nsudo nano /etc/default/kubelet\n```\nAjouter :\nKUBELET_EXTRA_ARGS=--node-ip=192.168.56.11  # changer l'IP selon le nÅ“u\nd\nPuis recharger le service :\n```bash\nsudo systemctl daemon-reexec\n```\n```bash\nsudo systemctl restart kubelet\n```\nğŸ“¥ TÃ©lÃ©charger les images Kubernetes\nSur tous les nÅ“uds / LoadBalancer :\nkubeadm config images pull --cri-socket unix:///run/containerd/containerd.so\nck\nğŸ§  Initialisation du Master1\nCluster kubernetes HA\n8\n\nSur k8s-master1  :\nkubeadm init \\\n  --apiserver-advertise-address=192.168.56.11 \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --control-plane-endpoint=192.168.56.14:6443 \\\n  --upload-certs \\\n  --cri-socket unix:///run/containerd/containerd.sock\nğŸ”‘ Ã€ noter aprÃ¨s lâ€™exÃ©cution :\nLe token pour rejoindre les nÅ“uds\nLe hash -discovery-token-ca-cert-hash\nLa clÃ© -certificate-key  pour les masters\nğŸ§© Configuration de kubectl  sur master1\nmkdir -p $HOME/.kube\n```bash\nsudo cp /etc/kubernetes/admin.conf $HOME/.kube/config\n```\n```bash\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\nğŸŒ Installation du rÃ©seau CNI (Weave Net)\nSur master1  :\n```bash\nkubectl apply -f https://github.com/weaveworks/weave/releases/download/v\n```\n2.8.1/weave-daemonset-k8s.\nğŸ§­ Ajouter les autres masters au cluster\nSur k8s-master2  (192.168.56.12) :\nCluster kubernetes HA\n9\n\nkubeadm join 192.168.56.14:6443 \\\n  --token <YOUR_TOKEN> \\\n  --discovery-token-ca-cert-hash sha256:<HASH> \\\n  --control-plane \\\n  --certificate-key <CERT_KEY> \\\n  --apiserver-advertise-address=192.168.56.12 \\\n  --cri-socket unix:///run/containerd/containerd.sock\nSur k8s-master3  (192.168.56.13) :\nkubeadm join 192.168.56.14:6443 \\\n  --token <YOUR_TOKEN> \\\n  --discovery-token-ca-cert-hash sha256:<HASH> \\\n  --control-plane \\\n  --certificate-key <CERT_KEY> \\\n  --apiserver-advertise-address=192.168.56.13 \\\n  --cri-socket unix:///run/containerd/containerd.sock\nğŸ‘· Ajouter le worker\nSur k8s-worker1  (192.168.56.17) :\nkubeadm join 192.168.56.14:6443 \\\n  --token <YOUR_TOKEN> \\\n  --discovery-token-ca-cert-hash sha256:<HASH> \\\n  --cri-socket unix:///run/containerd/containerd.sock\nâœ… VÃ©rification Finale sur Master1\n```bash\nkubectl get nodes -o wide\n```\n```bash\nkubectl get pods -A -o wide\n```\nCluster kubernetes HA\n10\n\nDocumentation â€“ Automatisation de \nlâ€™installation dâ€™un cluster Kubernetes avec \nAnsible\nğŸ“‚ Structure GÃ©nÃ©rale\nk8s-cluster-ansible/\nâ”œâ”€â”€ ansible.cfg\nâ”œâ”€â”€ group_vars/\nâ”‚   â””â”€â”€ all.yml\nâ”œâ”€â”€ inventory.yml\nâ”œâ”€â”€ keys/\nâ”œâ”€â”€ playbooks/\nâ”œâ”€â”€ roles/\nğŸ§© 1. ansible.cfg\nCe fichier configure le comportement global dâ€™Ansible.\n[defaults]\ninventory = inventory.yml     # Fichier dâ€™inventaire personnalisÃ©\nremote_user = root            # Utilisateur par dÃ©faut (souvent utilisÃ© en root via s\nudo)\nhost_key_checking = False     # DÃ©sactive la vÃ©rification du fingerprint SSH\nroles_path = ./roles          # Chemin vers les rÃ´les\nğŸŸ¡ Remarque : Ce fichier est global mais est surpassÃ© par les valeurs dÃ©finies \ndans inventory.yml  (comme ansible_user  et ansible_ssh_private_key_file ).\nğŸ—‚ï¸\nCluster kubernetes HA\n11\n\nğŸ—‚ï¸ 2. group_vars/all.yml\nCe fichier contient les variables globales Ã  tous les hÃ´tes.\nk8s_version: \"1.28.15\"                           # Version de Kubernetes Ã  installer\nk8s_apt_key: https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key\nk8s_apt_repo: \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gp\ng] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\"\ndocker_apt_key: https://download.docker.com/linux/ubuntu/gpg\ndocker_apt_repo: \"deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gp\ng] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release | l\nower }} stable\"\nğŸ“Œ UtilisÃ©es dans les rÃ´les comme containerd  et kubernetes  pour ajouter les dÃ©pÃ´ts.\nğŸ“‹ 3. inventory.yml\nCâ€™est le fichier dâ€™inventaire statique, qui dÃ©clare tous les nÅ“uds avec leurs \nadresses IP, utilisateurs SSH et chemins de clÃ©s privÃ©es.\nall:\n  vars:\n    ansible_user: vagrant\n    ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key\n  children:\n    masters:\n      hosts:\n        k8s-master1:\n          ansible_host: 192.168.56.11\n          ansible_user: vagrant\n          ansible_ssh_private_key_file: keys/master1\n          kubeadm_token: ...\n          discovery_hash: ...\n          certificate_key: ...\nCluster kubernetes HA\n12\n\n        k8s-master2:\n          ...\n        k8s-master3:\n          ...\n    workers:\n      hosts:\n        k8s-worker1:\n          ...\n    loadbalancers:\n      hosts:\n        k8s-lb1:\n          ...\nğŸ“Œ Les variables spÃ©cifiques ( token , hash , cert_key ) sont ajoutÃ©es uniquement sur \nk8s-master1  car câ€™est lui qui initie le cluster.\nğŸ” 4. keys/\nCe dossier contient les clÃ©s privÃ©es SSH Vagrant copiÃ©es depuis :\nC:\\vagrant-k8s-cluster\\.vagrant\\machines\\<vm>\\virtualbox\\private_key\nğŸ” Ces clÃ©s permettent Ã  Ansible dâ€™accÃ©der Ã  chaque VM sans mot de passe.\nğŸ—‚ï¸ 5. playbooks/\nCe dossier contiendra les playbooks Ansible pour automatiser chaque Ã©tape du \ndÃ©ploiement :\nsetup_cluster.yml  : prÃ©paration commune (disable swap, modules kernel, etc.)\nloadbalancer.yml  : configuration de HAProxy sur k8s-lb1\ninit_master.yml  : initialisation du premier master\njoin_master.yml  / join_worker.yml  : ajout des nÅ“uds au cluster\ndeploy_network.yml  : installation du CNI (ex. Weave Net)\nCluster kubernetes HA\n13\n\ndeploy_cluster.yml  : exÃ©cution de tous les playbooks en une seule commande\nconfigure_crictl.yml  : configuration de crictl\nreset_cluster.yml  : suppression complÃ¨te du cluster (utile en dev)\nğŸ“ 6. roles/\nRÃ©pertoire contenant les rÃ´les Ansible pour chaque composant :\nRÃ´le\nFonction principale\ncommon\nMise Ã  jour systÃ¨me, dÃ©sactivation swap, modules kernel, etc.\ncontainerd\nInstallation et configuration du runtime containerd\nkubernetes\nInstallation de kubelet, kubeadm, kubectl + sources APT\ninit_master\nInitialise k8s-master1 , gÃ©nÃ¨re la commande kubeadm join\njoin_master\nFait rejoindre k8s-master2/3  au cluster en mode control-plane\njoin_worker\nFait rejoindre les workers\nloadbalancer\nInstalle et configure haproxy  (via haproxy.cfg.j2 )\nreset_cluster\nRÃ©initialise complÃ¨tement les nÅ“uds Kubernetes\nğŸ§© RÃ´le common\nğŸ“ Cible : Tous les nÅ“uds (masters, workers, loadbalancer)\nğŸ”§ Objectif : Appliquer toutes les prÃ©requis systÃ¨me communs Ã  Kubernetes.\nğŸ”§ TÃ¢ches exÃ©cutÃ©es :\nOrdre\nTÃ¢che\nDescription\n1\nDÃ©sactiver le firewall\n(ufw)\nÃ‰vite les blocages de ports entre les nÅ“uds\n2\nMise Ã  jour complÃ¨te\nMise Ã  jour du cache et des paquets ( apt\nfull-upgrade )\n3\nInstaller systemd-\ntimesyncd\nSynchronisation automatique de lâ€™horloge\nCluster kubernetes HA\n14\n\n4\nActiver NTP\nActive la synchronisation de l'heure\n5\nDÃ©sactiver le swap\nRequis par Kubernetes (temporaire +\npermanent via /etc/fstab )\n6\nCharger les modules\nkernel requis\noverlay  et br_netfilter\n7\nConfigurer sysctl\nActive le forwarding IP et rÃ¨gles de pont\nrÃ©seau\n8\nInstaller les paquets de\nbase\ncurl, gpg, etc.\nâœ… RÃ©sultat attendu : Le systÃ¨me est prÃªt pour Kubernetes : pas de swap, rÃ©seau \nactivÃ©, modules chargÃ©s, heure Ã  jour.\nğŸ³ RÃ´le containerd\nğŸ“ Cible : Masters & Workers uniquement\nğŸ”§ Objectif : Installer et configurer containerd  (le runtime conteneur).\nğŸ”§ TÃ¢ches exÃ©cutÃ©es :\nOrdre\nTÃ¢che\nDescription\n1\nAjouter la clÃ© GPG Docker\nSÃ©curise lâ€™accÃ¨s au dÃ©pÃ´t\n2\nAjouter le dÃ©pÃ´t Docker\nPermet lâ€™installation de containerd  depuis\nDocker\n3\nInstaller containerd\nLe runtime container utilisÃ© par\nKubernetes\n4\nGÃ©nÃ©rer le fichier\nconfig.toml\nAvec containerd config default\n5\nConfigurer SystemdCgroup =\ntrue\nRequis pour compatibilitÃ© avec kubelet\n6\nRedÃ©marrer et activer\ncontainerd\nAppliquer la config dÃ¨s le boot\nâœ… RÃ©sultat attendu : containerd  fonctionne avec les bons paramÃ¨tres pour \nKubernetes ( SystemdCgroup  activÃ©).\nâ˜¸ï¸\nCluster kubernetes HA\n15\n\nâ˜¸ï¸ RÃ´le kubernetes\nğŸ“ Cible : Masters & Workers uniquement\nğŸ”§ Objectif : Installer les outils de base Kubernetes : kubelet , kubeadm , kubectl .\nğŸ”§ TÃ¢ches exÃ©cutÃ©es :\nOrdre\nTÃ¢che\nDescription\n1\nCrÃ©er /etc/apt/keyrings\nDossier pour stocker les clÃ©s\n2\nTÃ©lÃ©charger la clÃ© du repo\nKubernetes\nSignature de confiance\n3\nAjouter le dÃ©pÃ´t APT de\nKubernetes\nAccÃ¨s Ã  kubeadm , kubelet , etc.\n4\nMettre Ã  jour les paquets APT\nIntÃ¨gre le nouveau dÃ©pÃ´t\n5\nInstaller kubelet , kubeadm ,\nkubectl\nLes outils essentiels\n6\nHold  des versions\nEmpÃªche leur mise Ã  jour automatique\n(stabilitÃ©)\nâœ… RÃ©sultat attendu : Kubernetes est installÃ© et prÃªt Ã  Ãªtre initialisÃ© ou rejoint via \nkubeadm .\nğŸ§ª Playbooks de test\nCes playbooks permettent dâ€™exÃ©cuter chaque rÃ´le sÃ©parÃ©ment pour sâ€™assurer \nquâ€™ils fonctionnent bien.\nâ–¶ï¸ test_common.yml\n- name: Test common role\n  hosts: all\n  become: yes\n  roles:\n    - role: common\n      tags: common\nCluster kubernetes HA\n16\n\nâ¡ï¸ Applique le rÃ´le common  Ã  tous les nÅ“uds.\nâ–¶ï¸ test_containerd.yml\n- name: Test containerd role\n  hosts: masters,workers\n  become: true\n  tags: containerd\n  roles:\n    - containerd\nâ¡ï¸ Applique le rÃ´le containerd  uniquement aux masters et workers.\nâ–¶ï¸ test_kubernetes.yml\n- name: Test kubernetes role\n  hosts: masters,workers\n  become: true\n  tags: kubernetes\n  roles:\n    - kubernetes\nâ¡ï¸ Applique le rÃ´le kubernetes  uniquement aux masters et workers.\nğŸ§© RÃ´le loadbalancer\nğŸ“ Cible : k8s-lb1\nğŸ¯ Objectif : Installer et configurer HAProxy pour Ã©quilibrer la charge sur les \nmasters.\nğŸ“„ Fichier haproxy.cfg.j2\nContient la configuration HAProxy pour Ã©couter sur le port 6443  et rÃ©partir les \nrequÃªtes vers les 3 masters :\nCluster kubernetes HA\n17\n\nfrontend kubernetes-frontend\n    bind *:6443\n    mode tcp\n    option tcplog\n    default_backend kubernetes-backend\nbackend kubernetes-backend\n    mode tcp\n    balance roundrobin\n    option tcp-check\n    default-server inter 10s downinter 5s rise 2 fall 3\n    server k8s-master1 192.168.56.11:6443 check\n    server k8s-master2 192.168.56.12:6443 check\n    server k8s-master3 192.168.56.13:6443 check\nğŸ“œ TÃ¢ches loadbalancer/tasks/main.yml\n1. Installe HAProxy\n2. DÃ©ploie le fichier de config via un template Jinja\n3. RedÃ©marre et active le service\nğŸš€ RÃ´le init_master\nğŸ“ Cible : k8s-master1\nğŸ¯ Objectif : Initialiser le cluster Kubernetes.\nğŸ”§ Ã‰tapes :\n1. Tirer les images nÃ©cessaires avec kubeadm config images pull\n2. Lancer kubeadm init  avec :\nIP de lâ€™API : 192.168.56.14  (Load Balancer)\nCIDR du rÃ©seau Pod : 10.244.0.0/16  (Weave)\nCluster kubernetes HA\n18\n\nActivation du certificat partagÃ© -upload-certs\n3. Sauvegarde la sortie dans /root/kubeadm-init-output.txt\n4. Configure kubectl  pour root ( ~/.kube/config )\n5. GÃ©nÃ¨re un script gen_join_cmd.sh  via le template Jinja :\nExtrait et nettoie les commandes kubeadm join  du fichier texte\nGÃ©nÃ©re /root/join-master.sh  automatiquement exÃ©cutable\nğŸ” RÃ´le join_master\nğŸ“ Cible : k8s-master2  & k8s-master3\nğŸ¯ Objectif : Faire rejoindre les autres masters au cluster en mode control-plane.\nğŸ”§ Ã‰tapes :\n1. ExÃ©cute kubeadm join  avec :\nToken et hash fournis depuis k8s-master1\nClÃ© du certificat ( -certificate-key )\n2. Configure kubectl  ( ~/.kube/config ) pour debug/commande locale\nğŸ§± RÃ´le join_worker\nğŸ“ Cible : k8s-worker1\nğŸ¯ Objectif : Ajouter les nÅ“uds workers au cluster.\nğŸ”§ Ã‰tapes :\n1. ExÃ©cute kubeadm join  avec :\nToken et hash depuis k8s-master1\nIP publique du load balancer\n2. Ajoute kubelet.conf  dans ~/.kube/config  pour debug local\nğŸ› ï¸\nCluster kubernetes HA\n19\n\nğŸ› ï¸ Playbook configure_crictl.yml\nğŸ“ Cible : masters & workers\nğŸ¯ Objectif : Installer et configurer crictl  (outil pour interagir avec containerd).\nğŸ”§ Ã‰tapes :\n1. Installe cri-tools\n2. CrÃ©e /etc/crictl.  avec les bons chemins socket pour containerd\nğŸŒ Playbook deploy_network.yml\nğŸ“ Cible : k8s-master1\nğŸ¯ Objectif : Installer le plugin rÃ©seau Weave Net (CNI)\nğŸ”§ Ã‰tape unique :\nApplique le manifest officiel Weave Net :\n```bash\nkubectl apply -f https://github.com/weaveworks/weave/releases/download/v\n```\n2.8.1/weave-dae\nğŸ“˜ Extrait de la documentation\n# ğŸš€ Kubernetes HA Cluster avec Ansible\nCe projet permet de dÃ©ployer automatiquement un cluster Kubernetes haute d\nisponibilitÃ© (multi-master) avec un Load Balancer, `containerd` comme runtim\ne, et Weave Net comme plugin rÃ©seau.\nğŸ“¦ Technologies utilisÃ©es :\n- Ansible\n- Kubernetes v1.28\n- containerd\n- HAProxy (load balancing)\n- Weave Net (CNI)\nCluster kubernetes HA\n20\n\n## ğŸ“¥ Cloner ce dÃ©pÃ´t\n```\ngit clone https://github.com/motrabelsi10/k8s-ansible-cluster.git\ncd k8s-ansible-cluster\nğŸ§ª Structure du projet\ninventory.yml  : inventaire des nÅ“uds\ngroup_vars/all.yml  : variables globales (versions, dÃ©pÃ´ts)\nplaybooks/  : scripts Ansible organisÃ©s par Ã©tape\nroles/  : tÃ¢ches organisÃ©es par composant (common, containerd, kubernetes, \netc.)\nâ–¶ï¸ DÃ©ploiement complet\nPour exÃ©cuter tout le dÃ©ploiement automatiquement :\nansible-playbook playbooks/deploy_cluster.yml\nâš ï¸ Assure-toi que :\nLes clÃ©s SSH dans keys/  sont correctes\nLes VMs sont accessibles depuis la machine d'exÃ©cution\nAnsible est installÃ© sur ta machine\nCluster kubernetes HA\n21\n\nğŸš€ Ansible Kubernetes Helm Deployment â€” \nMySQL & Joget\nCe projet permet d'automatiser le dÃ©ploiement de MySQL puis Joget sur un \ncluster Kubernetes grÃ¢ce Ã  Helm et Ansible, en respectant les bonnes pratiques \n(rÃ´les, variables, tags, inventaire, etc.).\nğŸ“ Arborescence du projet\nansible-joget-deploy/\nâ”œâ”€â”€ ansible.cfg                 # Configuration Ansible\nâ”œâ”€â”€ inventory.yml              # Inventaire des hÃ´tes\nâ”œâ”€â”€ group_vars/\nâ”‚   â””â”€â”€ all.yml                # Variables globales\nâ”œâ”€â”€ charts/                    # (Optionnel) Chart local\nâ”‚   â””â”€â”€ mysql/                 # Chart Helm MySQL\nâ”œâ”€â”€ playbooks/\nâ”‚   â”œâ”€â”€ deploy_mysql.yml       # DÃ©ploie uniquement MySQL\nâ”‚   â”œâ”€â”€ deploy_joget.yml       # DÃ©ploie uniquement Joget\nâ”‚   â””â”€â”€ deploy_all.yml         # DÃ©ploie MySQL puis Joget\nâ””â”€â”€ roles/\n    â”œâ”€â”€ check_helm/            # VÃ©rifie que Helm est installÃ©\n    â”‚   â””â”€â”€ tasks/main.yml\n    â”œâ”€â”€ mysql/                 # DÃ©ploiement Helm de MySQL\n    â”‚   â””â”€â”€ tasks/main.yml\n    â””â”€â”€ joget/                 # DÃ©ploiement Helm de Joget\n        â””â”€â”€ tasks/main.yml\nğŸ”§ PrÃ©-requis\nCluster Kubernetes opÃ©rationnel (avec un nÅ“ud k8s-worker1 )\nCluster kubernetes HA\n22\n\nHelm installÃ© uniquement sur k8s-master1\nSSH sans mot de passe fonctionnel avec Ansible\nLes charts Helm mysql  et joget  disponibles dans :\n/home/vagrant/joget-helm/mysql\n/home/vagrant/joget-helm/joget\nğŸ“„ Fichier group_vars/all.yml\nnamespace: joget\nmysql_chart_path: /home/vagrant/joget-helm/mysql\njoget_chart_path: /home/vagrant/joget-helm/joget\nrelease_name_mysql: mysql\nrelease_name_joget: joget\nğŸ“„ Fichier inventory.yml\nall:\n  vars:\n    ansible_user: vagrant\n    ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key\n  children:\n    masters:\n      hosts:\n        k8s-master1:\n          ansible_host: 192.168.56.11\n          ansible_user: vagrant\n          ansible_ssh_private_key_file: /home/vagrant/k8s-cluster-ansible/keys/m\naster1\nğŸ“„\nCluster kubernetes HA\n23\n\nğŸ“„ Fichier ansible.cfg\n[defaults]\ninventory = inventory.yml\nremote_user = root\nhost_key_checking = False\nroles_path = ./roles\nğŸ“œ RÃ´le check_helm/tasks/main.yml\nVÃ©rifie si Helm est installÃ©, sinon Ã©choue.\n- name: Check if helm is installed\n  command: helm version\n  register: helm_check\n  ignore_errors: true\n- name: Fail if helm is not installed\n  fail:\n    msg: \"Helm n'est pas installÃ©. Veuillez lâ€™installer.\"\n  when: helm_check.rc != 0\nğŸ“œ RÃ´le mysql/tasks/main.yml\nDÃ©ploie MySQL dans le namespace joget  :\n- name: Deploy MySQL with Helm\n  command: >\n    helm upgrade --install {{ release_name_mysql }}\n    {{ mysql_chart_path }}\n    -n {{ namespace }} --create-namespace\n  tags: mysql\nCluster kubernetes HA\n24\n\nLe chart Helm cible spÃ©cifiquement k8s-worker1 via un \nnodeSelector dans values..\nğŸ“œ RÃ´le joget/tasks/main.yml\nDÃ©ploie Joget dans le mÃªme namespace :\n- name: Deploy Joget with Helm\n  command: >\n    helm upgrade --install {{ release_name_joget }}\n    {{ joget_chart_path }}\n    -n {{ namespace }}\n  tags: joget\nâ–¶ï¸ Playbook deploy_mysql.yml\n- name: Check Helm and Deploy MySQL\n  hosts: k8s-master1\n  become: true\n  roles:\n    - check_helm\n    - mysql\nâ–¶ï¸ Playbook deploy_joget.yml\n- name: Deploy Joget Application\n  hosts: k8s-master1\n  become: true\n  roles:\n    - check_helm\n    - joget\nâ–¶ï¸\nCluster kubernetes HA\n25\n\nâ–¶ï¸ Playbook deploy_all.yml\nDÃ©ploie MySQL puis Joget dans le bon ordre :\n- name: Deploy MySQL then Joget\n  hosts: k8s-master1\n  become: true\n  roles:\n    - check_helm\n    - mysql\n    - joget\nğŸ§ª ExÃ©cution des playbooks\nDÃ©ployer uniquement MySQL :\nansible-playbook playbooks/deploy_mysql.yml\nDÃ©ployer uniquement Joget :\nansible-playbook playbooks/deploy_joget.yml\nDÃ©ployer tout (MySQL puis Joget) :\nansible-playbook playbooks/deploy_all.yml\nâœ… VÃ©rification\nAprÃ¨s exÃ©cution, tu peux vÃ©rifier les pods :\n```bash\nkubectl get pods -n joget -o wide\n```\nCluster kubernetes HA\n26\n\nLes pods mysql  et joget  doivent Ãªtre en statut Running , et tourner sur k8s-worker1  si \nton nodeSelector  est bien configurÃ©.\nğŸ¯ RÃ´le du VIP (Virtual IP)\nLe VIP agit comme un point dâ€™accÃ¨s unique vers le cluster Kubernetes, en \nparticulier vers les API Servers des master  nodes.\nğŸ” Pourquoi câ€™est important ?\n1. Abstraction du backend\nTu nâ€™as pas besoin de connaÃ®tre lâ€™IP rÃ©elle des masters ( 192.168.56.11 , .12 , .13 ).\nTu utilises une seule IP (VIP), par exemple 192.168.56.30 .\n2. Ã‰quilibrage de charge\nLe VIP est associÃ© Ã  HAProxy sur les load balancers ( k8s-lb1 , k8s-lb2 ).\nLe trafic est rÃ©parti automatiquement entre les diffÃ©rents masters.\n3. Haute disponibilitÃ©\nSi k8s-lb1  tombe, Keepalived bascule le VIP vers k8s-lb2 .\nLe cluster continue de rÃ©pondre via la mÃªme IP, sans interruption.\nğŸ§  Exemple concret\nDans ta commande kubeadm init  :\nkubeadm init \\\n  --control-plane-endpoint=192.168.56.30:6443 \\\n  ...\nCluster kubernetes HA\n27\n\n192.168.56.30  est le VIP\nIl pointe vers les 2 LBs (grÃ¢ce Ã  Keepalived)\nEt les LBs redirigent vers les masters (grÃ¢ce Ã  HAProxy)\nâœ… 1. Objectif\nMettre en place deux load balancers avec :\nHAProxy : pour rÃ©partir la charge entre les 3 masters.\nKeepalived : pour fournir une IP virtuelle (VIP) en cas de panne de lâ€™un des LB.\nğŸ“ 2. Architecture\nVIP (ex: 192.168.56.30)\n   |\n   |-- k8s-lb1 (192.168.56.14) -- HAProxy\n   |-- k8s-lb2 (192.168.56.15) -- HAProxy\n        \\__ Cluster Masters: 192.168.56.11, .12, .13\nâš™ï¸ 3. Ã‰tapes pour chaque LB\nğŸ§© 3.1. Installer HAProxy et Keepalived\nCrÃ©er un rÃ´le Ansible loadbalancer  (ou configurer manuellement) :\n```bash\nsudo apt update && sudo apt install -y haproxy keepalived\n```\nğŸ“ 3.2. Configurer HAProxy /etc/haproxy/haproxy.cfg  :\nglobal\n  log /dev/log local0\n  maxconn 2000\n  daemon\nCluster kubernetes HA\n28\n\ndefaults\n  log global\n  mode tcp\n  option tcplog\n  timeout connect 10s\n  timeout client 1m\n  timeout server 1m\nfrontend kubernetes\n  bind *:6443\n  default_backend kubernetes-backend\nbackend kubernetes-backend\n  option httpchk GET /healthz\n  http-check expect status 200\n  server master1 192.168.56.11:6443 check\n  server master2 192.168.56.12:6443 check\n  server master3 192.168.56.13:6443 check\nRedÃ©marrer HAProxy :\n```bash\nsudo systemctl restart haproxy\n```\nğŸ›¡ï¸ 3.3. Configurer Keepalived /etc/keepalived/keepalived.conf\nSur k8s-lb1  (MASTER) :\nvrrp_instance VI_1 {\n    state MASTER\n    interface enp0s8\n    virtual_router_id 51\n    priority 101\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass secret\nCluster kubernetes HA\n29\n\n    }\n    virtual_ipaddress {\n        192.168.56.30\n    }\n}\nSur k8s-lb2  (BACKUP) :\nvrrp_instance VI_1 {\n    state BACKUP\n    interface enp0s8\n    virtual_router_id 51\n    priority 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass secret\n    }\n    virtual_ipaddress {\n        192.168.56.30\n    }\n}\nRedÃ©marrer Keepalived sur les deux :\n```bash\nsudo systemctl restart keepalived\n```\nâœ… 4. Tester\nPinger le VIP :\nping 192.168.56.30\nCluster kubernetes HA\n30\n\nVÃ©rifier que le port 6443 est ouvert :\ntelnet 192.168.56.30 6443\nPour simuler un failover :\n```bash\nsudo systemctl stop keepalived  # sur k8s-lb1\n```\nğŸ“¦ Documentation des Charts Helm â€“ Joget \n& MySQL\nğŸ” Quâ€™est-ce que Helm ?\nHelm est un gestionnaire de packages pour Kubernetes. Il fonctionne comme \napt  pour Ubuntu ou yum  pour CentOS, mais pour dÃ©ployer et gÃ©rer des \napplications dans Kubernetes.\nâœ… Avantages de Helm :\nğŸ“¦ Regroupe tous les fichiers de dÃ©ploiement dans un seul package appelÃ© \nchart\nğŸ” Permet des mises Ã  jour versionnÃ©es via helm upgrade\nğŸ› ï¸ Personnalisation facile via un fichier values.\nâ™»ï¸ RÃ©utilisable, portable, et facile Ã  partager\nğŸ“ Structure gÃ©nÃ©rale dâ€™un chart Helm\nmy-chart/\nâ”œâ”€â”€ Chart.            # MÃ©tadonnÃ©es du chart\nâ”œâ”€â”€ values.           # ParamÃ¨tres configurables\nCluster kubernetes HA\n31\n\nâ””â”€â”€ templates/            # Fichiers manifestes Kubernetes ( avec Go templates)\n    â”œâ”€â”€ deployment.\n    â”œâ”€â”€ service.\n    â”œâ”€â”€ pvc.\n    â”œâ”€â”€ pv.\n    â”œâ”€â”€ configmap.\n    â””â”€â”€ _helpers.tpl       # Fonctions utilitaires (optionnel)\nğŸ¯ Chart Helm â€“ Joget\nğŸ“„ Chart.\n```yaml\napiVersion: v2\nname: joget\ndescription: A Helm chart for Joget\nversion: 0.1.0\nappVersion: \"8.0\"\nDÃ©finit le nom du chart, sa version et la version de l'application Joget.\nğŸ“„ values.\nreplicaCount: 1\nimage:\n  repository: motrabelsi10/joget-v7\n  tag: latest\n  pullPolicy: Always\nmysql:\n  host: mysql.joget.svc.cluster.local\n  database: jwdb\n  username: tomcat\n  password: tomcat\nCluster kubernetes HA\n32\n\n```\nservice:\n  type: NodePort\n  portHttp: 8080\n  portHttps: 9080\n  nodePortHttp: 32325\nnodeSelector:\n  kubernetes.io/hostname: k8s-worker1\nDÃ©finit :\nLâ€™image Docker de Joget\nLa configuration de connexion MySQL\nLe type de service exposÃ© ( NodePort )\nLa planification du pod sur k8s-worker1\nğŸ“„ templates/deployment.\nDÃ©ploie Joget avec :\nUn replica\nDes variables dâ€™environnement pour la DB\nUn volume montÃ© depuis une PVC\nğŸ“„ templates/service.\nExpose Joget avec :\nDeux ports : HTTP (8080) & HTTPS (9080)\nNodePort  fixÃ© Ã  32325  (accÃ¨s depuis lâ€™extÃ©rieur)\nAffinitÃ© de session ( ClientIP )\nğŸ“„ templates/pv.\nCrÃ©e un volume persistant (PV) sur le worker  :\nCluster kubernetes HA\n33\n\nhostPath:\n  path: \"/mnt/data/joget\"\nAvec :\n2Gi de stockage\nReadWriteMany\nNode affinity sur k8s-worker1\nğŸ“„ templates/pvc.\nDemande un volume persistant de 2Gi liÃ© au PV prÃ©cÃ©dent.\nğŸ“„ templates/serviceaccount.\nUn ClusterRoleBinding  pour permettre Ã  Joget dâ€™accÃ©der Ã  certaines ressources \nKubernetes.\nğŸ§° Commandes Helm Essentielles\nğŸ“¥ 1. Installer un chart\nhelm install <release-name> <chart-path> -n <namespace> --create-namesp\nace\nğŸ” Exemple :\nhelm install joget ./joget -n joget --create-namespace\nhelm install mysql ./mysql -n joget --create-namespace\nâ™»ï¸\nCluster kubernetes HA\n34\n\nâ™»ï¸ 2. Mettre Ã  jour un chart dÃ©jÃ  installÃ© (upgrade)\nhelm upgrade <release-name> <chart-path> -n <namespace>\nğŸ” Exemple :\nhelm upgrade joget ./joget -n joget\nhelm upgrade mysql ./mysql -n joget\nğŸ—‘ï¸ 3. Supprimer un dÃ©ploiement Helm\nhelm uninstall <release-name> -n <namespace>\nğŸ” Exemple :\nhelm uninstall joget -n joget\nhelm uninstall mysql -n joget\nğŸ“„ 4. Lister les releases installÃ©es\nhelm list -A\nğŸ“Œ -A  montre toutes les releases dans tous les namespaces.\nğŸ” 5. Afficher les ressources installÃ©es par une \nrelease\nhelm get manifest <release-name> -n <namespace>\nğŸ” Exemple :\nCluster kubernetes HA\n35\n\nhelm get manifest joget -n joget\nğŸ§ª 6. Tester un chart localement (dry-run)\nhelm install <release-name> <chart-path> --dry-run --debug -n <namespace\n>\nğŸ” Exemple :\nhelm install joget ./joget --dry-run --debug -n joget\nğŸ“ 7. Afficher les valeurs par dÃ©faut dâ€™un chart\nhelm show values <chart-path>\nğŸ”§ 8. Personnaliser les valeurs avec un fichier\nhelm upgrade --install <release-name> <chart-path> -f my-values.yaml -n <n\namespace>\nCluster kubernetes HA\n36\n\n"
}